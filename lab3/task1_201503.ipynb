{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIDsQSAKq6dM",
        "outputId": "202207ff-ce6b-4a98-d654-63578f967d47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7fXz4-EJkp8",
        "outputId": "56950cb1-c6ce-4592-dcf5-6725c70dc217"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from evaluate import load"
      ],
      "metadata": {
        "id": "FonvZkoaGcHF"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy"
      ],
      "metadata": {
        "id": "8Dt0qYsUsEyr"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "metadata": {
        "id": "Hqu8u7PruZ7Z"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "IJXrLGUxrA8j"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glmv71XFS7-o",
        "outputId": "fb0ded82-2089-4de0-da45-d3f525f78e85"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "rlImQ5mQRZN_"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "TE8SSC7NRVy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_y_path = \"/content/drive/MyDrive/NLP Labs/data/shakespeare/valid.original.nltktok\"\n",
        "val_x_path = \"/content/drive/MyDrive/NLP Labs/data/shakespeare/valid.modern.nltktok\"\n",
        "train_y_path = \"/content/drive/MyDrive/NLP Labs/data/shakespeare/train.original.nltktok\"\n",
        "train_x_path = \"/content/drive/MyDrive/NLP Labs/data/shakespeare/train.modern.nltktok\"\n",
        "test_y_path = \"/content/drive/MyDrive/NLP Labs/data/shakespeare/test.original.nltktok\"\n",
        "test_x_path = \"/content/drive/MyDrive/NLP Labs/data/shakespeare/test.modern.nltktok\""
      ],
      "metadata": {
        "id": "DDZp-Jfo_luQ"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_y = pd.read_table(val_y_path, header=None, names=[\"Sentences\"])\n",
        "val_x = pd.read_table(val_x_path, header=None, names=[\"Sentences\"])\n",
        "test_y = pd.read_table(test_y_path, header=None, names=[\"Sentences\"])\n",
        "test_x = pd.read_table(test_x_path, header=None, names=[\"Sentences\"])\n",
        "train_y = pd.read_table(train_y_path, header=None, names=[\"Sentences\"])\n",
        "train_x = pd.read_table(train_x_path, header=None, names=[\"Sentences\"])"
      ],
      "metadata": {
        "id": "iJaVORktGCey"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = pd.concat([val_x, pd.concat([test_x, test_x])])"
      ],
      "metadata": {
        "id": "rD9snre2RvEI"
      },
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y = pd.concat([val_y, pd.concat([test_y, test_y])])"
      ],
      "metadata": {
        "id": "Yxw2YwGWR0s6"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Functions"
      ],
      "metadata": {
        "id": "_GAMeNeHd_q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data):\n",
        "    data['Sentences_Tokens'] = data['Sentences'].apply(lambda x: word_tokenize(x.lower(), language='english'))"
      ],
      "metadata": {
        "id": "N4wzyvZxPt-J"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_start_end_token(data):\n",
        "    data['Sentences_Tokens'] = data['Sentences_Tokens'].apply(lambda x: np.concatenate((['<START>'], x, ['<END>'])))"
      ],
      "metadata": {
        "id": "IUICqte5Ree1"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_to_index(data, w_to_i):\n",
        "    data['Sentences_Index'] = data['Sentences_Tokens'].apply(lambda x: [w_to_i[word] for word in x])"
      ],
      "metadata": {
        "id": "qh-Z7pt_SRAD"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        vocab.update(sentence)\n",
        "    vocab = list(vocab)\n",
        "    w_to_i = {word: index for index, word in enumerate(vocab)}\n",
        "    i_to_w = {index: word for index, word in enumerate(vocab)}\n",
        "\n",
        "    return vocab, w_to_i, i_to_w"
      ],
      "metadata": {
        "id": "yZcQCAxiRjs2"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(padding_size, vocabulary_size_x, vocabulary_size_y, embedding_size):\n",
        "    encoder_inputs = Input(shape=(padding_size,))\n",
        "    encoder_embedding = Embedding(input_dim=vocabulary_size_x,\n",
        "                                  output_dim=embedding_size)(encoder_inputs)\n",
        "    encoder = LSTM(128, return_state=True)\n",
        "    _, state_h, state_c = encoder(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(padding_size,))\n",
        "    decoder_embedding = Embedding(input_dim=vocabulary_size_y, output_dim=embedding_size,\n",
        "                                  trainable=False)(decoder_inputs)\n",
        "    decoder = LSTM(128, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder(decoder_embedding,\n",
        "                                    initial_state=encoder_states)\n",
        "\n",
        "    decoder_outputs = Dense(vocabulary_size_y, activation='softmax')(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs],\n",
        "                  decoder_outputs)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
        "                  loss=categorical_crossentropy)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "jwFAyp20JiSU"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(sentences, id_to_word):\n",
        "    out_sentences = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        out_sentences.append(' '.join([id_to_word[s] for s in sent]))\n",
        "\n",
        "    return out_sentences"
      ],
      "metadata": {
        "id": "yuh1bH2IPY9w"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_data(sentences, translations):\n",
        "    input_sentences, input_translations, next_words = [], [], []\n",
        "    for sentence, rephrase in zip(sentences, translations):\n",
        "        for i in range(1, len(rephrase)):\n",
        "            input_sentences.append(sentence)\n",
        "            input_translations.append(rephrase[:i])\n",
        "            next_words.append(rephrase[i])\n",
        "    return input_sentences, input_translations, next_words"
      ],
      "metadata": {
        "id": "FqMEUUGntLOa"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(model, input_sent, word_to_id, padding_size):\n",
        "    generated_sent = [word_to_id['<START>']]\n",
        "\n",
        "    for i in range(padding_size):\n",
        "        output_sent = pad_sequences([generated_sent], padding_size)\n",
        "        predictions = model.predict([np.expand_dims(input_sent, axis=0), output_sent])\n",
        "        next_word = np.argmax(predictions)\n",
        "        generated_sent.append(next_word)\n",
        "\n",
        "    return generated_sent"
      ],
      "metadata": {
        "id": "TsHvP_KmPXl0"
      },
      "execution_count": 505,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_alphabetic(text):\n",
        "    clean_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "zq6s5bhEJCK5"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vocabulary"
      ],
      "metadata": {
        "id": "KnezQUqXSCPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = df_x.applymap(remove_non_alphabetic)\n",
        "df_y = df_y.applymap(remove_non_alphabetic)"
      ],
      "metadata": {
        "id": "c1LnUTutK-42"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(df_x)\n",
        "tokenize(df_y)"
      ],
      "metadata": {
        "id": "DuNlVY2PSahx"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "append_start_end_token(df_x)\n",
        "append_start_end_token(df_y)"
      ],
      "metadata": {
        "id": "_dn5pkNgSyaT"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_x[\"Sentences_Tokens\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is2JB_NXKPnG",
        "outputId": "eb1b1a5c-5cd9-45a7-cc9b-0f4e5920427c"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [<START>, now, you, lie, there, on, the, path,...\n",
              "1       [<START>, she, said, if, she, were, interested...\n",
              "2       [<START>, besides, she, treats, me, more, resp...\n",
              "3       [<START>, whats, the, obvious, conclusion, fro...\n",
              "4       [<START>, just, think, i, could, be, count, ma...\n",
              "                              ...                        \n",
              "1457               [<START>, thats, good, my, boy, <END>]\n",
              "1458        [<START>, but, where, have, you, been, <END>]\n",
              "1459    [<START>, ill, tell, you, before, you, have, t...\n",
              "1460    [<START>, you, have, the, sacred, power, to, c...\n",
              "1461    [<START>, i, carry, no, hatred, holy, man, bec...\n",
              "Name: Sentences_Tokens, Length: 4142, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_x, w_to_i_x, i_to_w_x = create_vocabulary(df_x['Sentences_Tokens'].values.tolist())\n",
        "vocab_y, w_to_i_y, i_to_w_y = create_vocabulary(df_y['Sentences_Tokens'].values.tolist())"
      ],
      "metadata": {
        "id": "6hsOKHXUSGEV"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# w_to_i_x"
      ],
      "metadata": {
        "id": "T51_eF1bPRKv"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_to_index(df_x, w_to_i_x)\n",
        "map_to_index(df_y, w_to_i_y)\n",
        "indices_x = df_x['Sentences_Index'].values.tolist()\n",
        "indices_y = df_y['Sentences_Index'].values.tolist()"
      ],
      "metadata": {
        "id": "h3TMDoCHTM0M"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indices_x"
      ],
      "metadata": {
        "id": "8A9MBsgeU1RG"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "WVYhQJy6U2ap"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(indices_x, indices_y,\n",
        "                                                            test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "p1ksoCtUVPat"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_word_length = sum(len(sentence) for sentence in df_x['Sentences_Tokens']) / len(df_x['Sentences_Tokens'])"
      ],
      "metadata": {
        "id": "_hvfSAyZV-Ml"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_word_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTcm9dFgWOc6",
        "outputId": "e494676c-d475-4376-df4c-a703a55acbdd"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.420328343795267"
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding_size = 10"
      ],
      "metadata": {
        "id": "nh09P4nFWZhI"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_x, input_y, next_words = create_train_data(train_x, train_y)"
      ],
      "metadata": {
        "id": "f-38mQB-tMeb"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_x_padded = pad_sequences(input_x, padding_size)\n",
        "input_y_padded = pad_sequences(input_y, padding_size)"
      ],
      "metadata": {
        "id": "A-WBa6bqVhAL"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_y_padded[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4NYHL76M_8B",
        "outputId": "7381874a-259e-4577-b5ff-2a7b465a8cfc"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0, 2652],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0, 2652, 1879],\n",
              "       [   0,    0,    0,    0,    0,    0,    0, 2652, 1879, 1394],\n",
              "       [   0,    0,    0,    0,    0,    0, 2652, 1879, 1394, 3064],\n",
              "       [   0,    0,    0,    0,    0, 2652, 1879, 1394, 3064,  938],\n",
              "       [   0,    0,    0,    0, 2652, 1879, 1394, 3064,  938, 2424],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 2652],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0, 2652, 2099],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 2652],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0, 2652, 2184]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_words[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aBq2CesRo1e",
        "outputId": "3e7c0985-35d1-4cc6-e64d-ed0054d8ceb5"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1879, 1394, 3064, 938, 2424]"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "4S_BPApFrvtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 512"
      ],
      "metadata": {
        "id": "mCrCdGtrr2iG"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(padding_size, len(vocab_x), len(vocab_y), embedding_size)"
      ],
      "metadata": {
        "id": "cdfDS1QrrvKQ"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_binarizer = LabelBinarizer()\n",
        "label_binarizer.fit_transform(list(w_to_i_y.values()))\n",
        "next_words = label_binarizer.transform(next_words)"
      ],
      "metadata": {
        "id": "J_6ZAwwvuTHR"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_y_padded[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyiHa4XymVMX",
        "outputId": "9a96c703-3f6d-41f0-fe2e-cc744540347e"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0, 2652],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0, 2652, 1879],\n",
              "       [   0,    0,    0,    0,    0,    0,    0, 2652, 1879, 1394],\n",
              "       [   0,    0,    0,    0,    0,    0, 2652, 1879, 1394, 3064],\n",
              "       [   0,    0,    0,    0,    0, 2652, 1879, 1394, 3064,  938]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 340
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decided on batch_size of 64 as I read that its used as a general value, but after reading: https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU, I decided to try the value of 16. Event though 16 will take double the time of 64, this paper suggests the best results and I want to test that.\n",
        "- Started with 10 epochs, it was clear that it was a very far from the ideal number of epochs, so I increased to 50\n",
        "- 50 showed improvements, but the predictions were still incoherent.\n",
        "- Decided to go with 500 epochs to test what happens with even bigger values for epoch size. After failing mid way through the execution which was supposed to last 3 and a half hours, I decided to stay try with 100 epochs.\n",
        "- After trying with 100 epochs, the performance was proven better with 50 epochs."
      ],
      "metadata": {
        "id": "lnDbJmmo0D4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit([input_x_padded, input_y_padded],\n",
        "#               next_words,\n",
        "#               epochs=100, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOiYhCB9syno",
        "outputId": "8f07b16f-4797-4b53-f307-54ca1fa3c2c8"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2295/2295 [==============================] - 19s 8ms/step - loss: 1.1256\n",
            "Epoch 2/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.9318\n",
            "Epoch 3/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.9519\n",
            "Epoch 4/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.9473\n",
            "Epoch 5/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.9267\n",
            "Epoch 6/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8935\n",
            "Epoch 7/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8952\n",
            "Epoch 8/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 0.9061\n",
            "Epoch 9/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 0.8760\n",
            "Epoch 10/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8557\n",
            "Epoch 11/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8803\n",
            "Epoch 12/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8283\n",
            "Epoch 13/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8482\n",
            "Epoch 14/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8466\n",
            "Epoch 15/100\n",
            "2295/2295 [==============================] - 16s 7ms/step - loss: 0.7809\n",
            "Epoch 16/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7993\n",
            "Epoch 17/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7911\n",
            "Epoch 18/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7836\n",
            "Epoch 19/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7856\n",
            "Epoch 20/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.9482\n",
            "Epoch 21/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 0.8167\n",
            "Epoch 22/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 0.7496\n",
            "Epoch 23/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 0.7757\n",
            "Epoch 24/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 0.7761\n",
            "Epoch 25/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7856\n",
            "Epoch 26/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.8059\n",
            "Epoch 27/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7803\n",
            "Epoch 28/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7574\n",
            "Epoch 29/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 0.7744\n",
            "Epoch 30/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 4.9664\n",
            "Epoch 31/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 8.9881\n",
            "Epoch 32/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 7.8905\n",
            "Epoch 33/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 6.9392\n",
            "Epoch 34/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 7.1401\n",
            "Epoch 35/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 7.1170\n",
            "Epoch 36/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 6.1032\n",
            "Epoch 37/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 6.0402\n",
            "Epoch 38/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 5.8603\n",
            "Epoch 39/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 5.4067\n",
            "Epoch 40/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 5.7969\n",
            "Epoch 41/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 6.5197\n",
            "Epoch 42/100\n",
            "2295/2295 [==============================] - 19s 8ms/step - loss: 5.8490\n",
            "Epoch 43/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 5.8813\n",
            "Epoch 44/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 6.2200\n",
            "Epoch 45/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 5.9792\n",
            "Epoch 46/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 5.7618\n",
            "Epoch 47/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 5.4044\n",
            "Epoch 48/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 5.0613\n",
            "Epoch 49/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.9131\n",
            "Epoch 50/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 4.8026\n",
            "Epoch 51/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.7174\n",
            "Epoch 52/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.7386\n",
            "Epoch 53/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 5.1453\n",
            "Epoch 54/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 5.2956\n",
            "Epoch 55/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 5.0590\n",
            "Epoch 56/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 5.5996\n",
            "Epoch 57/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 5.2071\n",
            "Epoch 58/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.9172\n",
            "Epoch 59/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 4.7230\n",
            "Epoch 60/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 4.5900\n",
            "Epoch 61/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.5571\n",
            "Epoch 62/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.5125\n",
            "Epoch 63/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 4.4545\n",
            "Epoch 64/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.5075\n",
            "Epoch 65/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.4266\n",
            "Epoch 66/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.3770\n",
            "Epoch 67/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.4437\n",
            "Epoch 68/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.3114\n",
            "Epoch 69/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.5404\n",
            "Epoch 70/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.5213\n",
            "Epoch 71/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.4893\n",
            "Epoch 72/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.2719\n",
            "Epoch 73/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 4.2121\n",
            "Epoch 74/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 4.1143\n",
            "Epoch 75/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.9858\n",
            "Epoch 76/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.9123\n",
            "Epoch 77/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.8442\n",
            "Epoch 78/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.8425\n",
            "Epoch 79/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.7520\n",
            "Epoch 80/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.6613\n",
            "Epoch 81/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.6059\n",
            "Epoch 82/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.5169\n",
            "Epoch 83/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 3.5707\n",
            "Epoch 84/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.7626\n",
            "Epoch 85/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.7337\n",
            "Epoch 86/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.6822\n",
            "Epoch 87/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.6793\n",
            "Epoch 88/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 4.1174\n",
            "Epoch 89/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.7180\n",
            "Epoch 90/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.6240\n",
            "Epoch 91/100\n",
            "2295/2295 [==============================] - 17s 7ms/step - loss: 3.6870\n",
            "Epoch 92/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.6129\n",
            "Epoch 93/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 3.4890\n",
            "Epoch 94/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 3.4553\n",
            "Epoch 95/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 3.5263\n",
            "Epoch 96/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.4310\n",
            "Epoch 97/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 3.3544\n",
            "Epoch 98/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.3109\n",
            "Epoch 99/100\n",
            "2295/2295 [==============================] - 17s 8ms/step - loss: 3.3283\n",
            "Epoch 100/100\n",
            "2295/2295 [==============================] - 18s 8ms/step - loss: 3.2479\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e2398d62590>"
            ]
          },
          "metadata": {},
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_weights('/content/drive/MyDrive/NLP Labs/models/EncoderDecoder_13.h5')"
      ],
      "metadata": {
        "id": "dWo2Y8-4sxL8"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/NLP Labs/models/EncoderDecoder_12.h5')"
      ],
      "metadata": {
        "id": "_TfoWYHEsWY2"
      },
      "execution_count": 458,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decode"
      ],
      "metadata": {
        "id": "wB8JKqUadeaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_y = []"
      ],
      "metadata": {
        "id": "Z8VASHUIrj6q"
      },
      "execution_count": 510,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sentences_x = pad_sequences(test_x, padding_size)"
      ],
      "metadata": {
        "id": "wB93NfEquC4Y"
      },
      "execution_count": 511,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduced the number of predicted sentences to 20 for debugging purposes"
      ],
      "metadata": {
        "id": "P-MtBChkXhNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in padded_sentences_x[:20]:\n",
        "        pred = decode(model, sentence, w_to_i_y, padding_size)\n",
        "        output_y.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABtZfpldrsOq",
        "outputId": "4c746c7e-2d88-4fec-a980-b93bdc508228"
      },
      "execution_count": 512,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_x = convert(test_x, i_to_w_x)\n",
        "output_y_gt = convert(test_y, i_to_w_y)\n",
        "output_y_pred = convert(output_y, i_to_w_y)\n",
        "# output_y_pred = [[i_to_w_y[w] for w in sent] for sent in output_y]"
      ],
      "metadata": {
        "id": "ReLd_dmy2hTf"
      },
      "execution_count": 513,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for in_x, gt_y, pred_y in zip(input_x, output_y_gt, output_y_pred):\n",
        "#         print(f'Input sentence: {in_x}')\n",
        "#         print(f'GT translation: {gt_y}')\n",
        "#         print(f'Pred translation: {pred_y}')\n",
        "#         print()"
      ],
      "metadata": {
        "id": "mJuHWSSa29lr"
      },
      "execution_count": 516,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load"
      ],
      "metadata": {
        "id": "n0FQlBsEfZEG"
      },
      "execution_count": 514,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load('bleu')\n",
        "results = metric.compute(predictions=output_y_pred, references=output_y_gt[:20])\n",
        "score = results['bleu']\n",
        "print(f'BLEU score: {score}')"
      ],
      "metadata": {
        "id": "fEpCzBwJIqKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d55d77c-6db9-4ae6-ab76-fb75e6a9f47f"
      },
      "execution_count": 517,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.07271793025092739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import single_meteor_score"
      ],
      "metadata": {
        "id": "pcAQcxRNLwGQ"
      },
      "execution_count": 518,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate import meteor\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "6zObRM0MWuX6"
      },
      "execution_count": 526,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnCjOEiCXCcE",
        "outputId": "d0fe6613-7e9f-4667-dd49-d50effacb13d"
      },
      "execution_count": 528,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 528
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_score = 0.0\n",
        "for hyp, ref in zip(output_y_gt[:20], output_y_pred):\n",
        "        m_score += round(meteor([word_tokenize(hyp)], word_tokenize(ref)), 4)"
      ],
      "metadata": {
        "id": "qLoZNA8gWKms"
      },
      "execution_count": 529,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(m_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uadnI_SmLopd",
        "outputId": "d876dab1-db5c-4834-d989-48a6c37d905f"
      },
      "execution_count": 531,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.977399999999999\n"
          ]
        }
      ]
    }
  ]
}