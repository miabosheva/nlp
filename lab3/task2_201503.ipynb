{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "FonvZkoaGcHF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8Dt0qYsUsEyr"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE8SSC7NRVy8"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "DDZp-Jfo_luQ"
   },
   "outputs": [],
   "source": [
    "val_y_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/shakespeare/valid.original.nltktok\"\n",
    "val_x_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/shakespeare/valid.modern.nltktok\"\n",
    "train_y_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/shakespeare/train.original.nltktok\"\n",
    "train_x_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/shakespeare/train.modern.nltktok\"\n",
    "test_y_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/shakespeare/test.original.nltktok\"\n",
    "test_x_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/shakespeare/test.modern.nltktok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "iJaVORktGCey"
   },
   "outputs": [],
   "source": [
    "val_y = pd.read_table(val_y_path, header=None, names=[\"Sentences\"])\n",
    "val_x = pd.read_table(val_x_path, header=None, names=[\"Sentences\"])\n",
    "test_y = pd.read_table(test_y_path, header=None, names=[\"Sentences\"])\n",
    "test_x = pd.read_table(test_x_path, header=None, names=[\"Sentences\"])\n",
    "train_y = pd.read_table(train_y_path, header=None, names=[\"Sentences\"])\n",
    "train_x = pd.read_table(train_x_path, header=None, names=[\"Sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rD9snre2RvEI",
    "outputId": "87b2da81-69c1-4699-fe1e-a33ac8776430"
   },
   "outputs": [],
   "source": [
    "df_x = pd.concat([pd.concat([train_x, test_x]), val_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yxw2YwGWR0s6",
    "outputId": "7b233e58-e719-491b-c2b3-f4447eb97277"
   },
   "outputs": [],
   "source": [
    "df_y = pd.concat([pd.concat([train_y, test_y]), val_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GAMeNeHd_q6"
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def create_transformers_train_data(sentences, translations, tokenizer):\n",
    "    inputs_en = tokenizer(sentences, max_length=10, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        outputs_es = tokenizer(translations, max_length=10, truncation=True)\n",
    "\n",
    "    data = Dataset.from_dict({'input_ids': inputs_en['input_ids'],\n",
    "                              'attention_mask': inputs_en['attention_mask'],\n",
    "                              'labels': outputs_es['input_ids']})\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def decode_with_transformer(sentence, tokenizer, model):\n",
    "    tokens = tokenizer([sentence], return_tensors='np')\n",
    "    out = model.generate(**tokens, max_length=10)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        pred_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    return pred_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnezQUqXSCPp"
   },
   "source": [
    "# Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "prefix = 'translate from English to Shakespearean: '\n",
    "model_name = 't5-small'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "sentences_x = [prefix + sentence for sentence in df_x['Sentences'].values.tolist()]\n",
    "sentences_y = df_y['Sentences'].values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "p1ksoCtUVPat"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(sentences_x, sentences_y,\n",
    "                                                            test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S_BPApFrvtx"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mia\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_set = create_transformers_train_data(train_x, train_y, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4511a1dac89f4b739a6c26599faa9381"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b425676bd0844b8a9bb450084c9d418"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                           model=model_name,\n",
    "                                           return_tensors='tf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train_set = model.prepare_tf_dataset(train_set, collate_fn=data_collator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=0.01))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2370/2370 [==============================] - 1087s 451ms/step - loss: 4.8732\n",
      "Epoch 2/5\n",
      "2370/2370 [==============================] - 1064s 449ms/step - loss: 4.8537\n",
      "Epoch 3/5\n",
      "2370/2370 [==============================] - 1107s 467ms/step - loss: 4.8690\n",
      "Epoch 4/5\n",
      "2370/2370 [==============================] - 1117s 471ms/step - loss: 4.6937\n",
      "Epoch 5/5\n",
      "2370/2370 [==============================] - 1091s 460ms/step - loss: 4.7139\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1601a977520>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, final_layer_norm_layer_call_fn, final_layer_norm_layer_call_and_return_conditional_losses, dropout_24_layer_call_fn, dropout_24_layer_call_and_return_conditional_losses while saving (showing 5 of 525). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/models\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"C:/Users/Mia/Desktop/FINKI/NLP/nlp/lab3/models\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer \"SelfAttention\" \"                 f\"(type TFT5Attention).\n\n{{function_node __wrapped__XlaDynamicSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Could not find compiler for platform Host: NOT_FOUND: could not find registered compiler for platform Host -- was support for that platform linked in? [Op:XlaDynamicSlice]\n\nCall arguments received by layer \"SelfAttention\" \"                 f\"(type TFT5Attention):\n  • hidden_states=tf.Tensor(shape=(1, 1, 512), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 1, 1, 2), dtype=float32)\n  • key_value_states=None\n  • position_bias=None\n  • past_key_value=('tf.Tensor(shape=(1, 8, 1, 64), dtype=float32)', 'tf.Tensor(shape=(1, 8, 1, 64), dtype=float32)')\n  • layer_head_mask=None\n  • query_length=None\n  • use_cache=True\n  • training=False\n  • output_attentions=False",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[81], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m output_y \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m test_x:\n\u001B[1;32m----> 3\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mdecode_with_transformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     output_y\u001B[38;5;241m.\u001B[39mappend(pred)\n",
      "Cell \u001B[1;32mIn[59], line 3\u001B[0m, in \u001B[0;36mdecode_with_transformer\u001B[1;34m(sentence, tokenizer, model)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_with_transformer\u001B[39m(sentence, tokenizer, model):\n\u001B[0;32m      2\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m tokenizer([sentence], return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnp\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m     out \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokens, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mas_target_tokenizer():\n\u001B[0;32m      6\u001B[0m         pred_sentence \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(out[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\tf_utils.py:903\u001B[0m, in \u001B[0;36mTFGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001B[0m\n\u001B[0;32m    898\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    899\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_return_sequences has to be 1, but is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m when doing\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    900\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m greedy search.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    901\u001B[0m         )\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[1;32m--> 903\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgreedy_search(\n\u001B[0;32m    904\u001B[0m         input_ids,\n\u001B[0;32m    905\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[0;32m    906\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[0;32m    907\u001B[0m         eos_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[0;32m    908\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[0;32m    909\u001B[0m         output_scores\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_scores,\n\u001B[0;32m    910\u001B[0m         return_dict_in_generate\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mreturn_dict_in_generate,\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    912\u001B[0m     )\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_contrastive_search_gen_mode:\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mnum_return_sequences \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\tf_utils.py:1732\u001B[0m, in \u001B[0;36mTFGenerationMixin.greedy_search\u001B[1;34m(self, input_ids, max_length, pad_token_id, eos_token_id, logits_processor, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001B[0m\n\u001B[0;32m   1729\u001B[0m \u001B[38;5;66;03m# 2-to-n generation steps can then be run in autoregressive fashion\u001B[39;00m\n\u001B[0;32m   1730\u001B[0m \u001B[38;5;66;03m# only in case 1st generation step does NOT yield EOS token though\u001B[39;00m\n\u001B[0;32m   1731\u001B[0m maximum_iterations \u001B[38;5;241m=\u001B[39m max_length \u001B[38;5;241m-\u001B[39m cur_len\n\u001B[1;32m-> 1732\u001B[0m generated, _, cur_len, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhile_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1733\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgreedy_search_cond_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgreedy_search_body_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1735\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinished_sequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1737\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1739\u001B[0m \u001B[38;5;66;03m# 6. prepare outputs\u001B[39;00m\n\u001B[0;32m   1740\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_xla:\n\u001B[0;32m   1741\u001B[0m     \u001B[38;5;66;03m# cut for backward compatibility\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629\u001B[0m, in \u001B[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    622\u001B[0m           _PRINTED_WARNING[(func, arg_name)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    623\u001B[0m         logging\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    624\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrom \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: calling \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) with \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is deprecated and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    625\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwill be removed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mInstructions for updating:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    626\u001B[0m             _call_location(), decorator_utils\u001B[38;5;241m.\u001B[39mget_qualified_name(func),\n\u001B[0;32m    627\u001B[0m             func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m, arg_name, arg_value, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124min a future version\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    628\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m date \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m date), instructions)\n\u001B[1;32m--> 629\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2513\u001B[0m, in \u001B[0;36mwhile_loop_v2\u001B[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001B[0m\n\u001B[0;32m   2337\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhile_loop\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[0;32m   2338\u001B[0m \u001B[38;5;129m@deprecation\u001B[39m\u001B[38;5;241m.\u001B[39mdeprecated_arg_values(\n\u001B[0;32m   2339\u001B[0m     \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2354\u001B[0m                   maximum_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2355\u001B[0m                   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2356\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001B[39;00m\n\u001B[0;32m   2357\u001B[0m \n\u001B[0;32m   2358\u001B[0m \u001B[38;5;124;03m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2511\u001B[0m \n\u001B[0;32m   2512\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2513\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwhile_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2514\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcond\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2515\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2516\u001B[0m \u001B[43m      \u001B[49m\u001B[43mloop_vars\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloop_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2517\u001B[0m \u001B[43m      \u001B[49m\u001B[43mshape_invariants\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshape_invariants\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2518\u001B[0m \u001B[43m      \u001B[49m\u001B[43mparallel_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2519\u001B[0m \u001B[43m      \u001B[49m\u001B[43mback_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mback_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2520\u001B[0m \u001B[43m      \u001B[49m\u001B[43mswap_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswap_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2521\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2522\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmaximum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2523\u001B[0m \u001B[43m      \u001B[49m\u001B[43mreturn_same_structure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2762\u001B[0m, in \u001B[0;36mwhile_loop\u001B[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001B[0m\n\u001B[0;32m   2759\u001B[0m loop_var_structure \u001B[38;5;241m=\u001B[39m nest\u001B[38;5;241m.\u001B[39mmap_structure(type_spec\u001B[38;5;241m.\u001B[39mtype_spec_from_value,\n\u001B[0;32m   2760\u001B[0m                                         \u001B[38;5;28mlist\u001B[39m(loop_vars))\n\u001B[0;32m   2761\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m cond(\u001B[38;5;241m*\u001B[39mloop_vars):\n\u001B[1;32m-> 2762\u001B[0m   loop_vars \u001B[38;5;241m=\u001B[39m \u001B[43mbody\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mloop_vars\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2763\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m try_to_pack \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loop_vars, (\u001B[38;5;28mlist\u001B[39m, _basetuple)):\n\u001B[0;32m   2764\u001B[0m     packed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2753\u001B[0m, in \u001B[0;36mwhile_loop.<locals>.<lambda>\u001B[1;34m(i, lv)\u001B[0m\n\u001B[0;32m   2750\u001B[0m     loop_vars \u001B[38;5;241m=\u001B[39m (counter, loop_vars)\n\u001B[0;32m   2751\u001B[0m     cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, lv: (  \u001B[38;5;66;03m# pylint: disable=g-long-lambda\u001B[39;00m\n\u001B[0;32m   2752\u001B[0m         math_ops\u001B[38;5;241m.\u001B[39mlogical_and(i \u001B[38;5;241m<\u001B[39m maximum_iterations, orig_cond(\u001B[38;5;241m*\u001B[39mlv)))\n\u001B[1;32m-> 2753\u001B[0m     body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m i, lv: (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[43morig_body\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlv\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   2754\u001B[0m   try_to_pack \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   2756\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m executing_eagerly:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\tf_utils.py:1653\u001B[0m, in \u001B[0;36mTFGenerationMixin.greedy_search.<locals>.greedy_search_body_fn\u001B[1;34m(generated, finished_sequences, cur_len, model_kwargs)\u001B[0m\n\u001B[0;32m   1651\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, use_cache\u001B[38;5;241m=\u001B[39muse_cache, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;66;03m# forward pass to get next token logits\u001B[39;00m\n\u001B[1;32m-> 1653\u001B[0m model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[0;32m   1655\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   1656\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   1657\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   1658\u001B[0m )\n\u001B[0;32m   1659\u001B[0m next_token_logits \u001B[38;5;241m=\u001B[39m model_outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   1661\u001B[0m \u001B[38;5;66;03m# pre-process distribution\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py:426\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    423\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    425\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[1;32m--> 426\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munpacked_inputs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:1358\u001B[0m, in \u001B[0;36mTFT5ForConditionalGeneration.call\u001B[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m   1355\u001B[0m     decoder_input_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shift_right(labels)\n\u001B[0;32m   1357\u001B[0m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[1;32m-> 1358\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1362\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1363\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1364\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1366\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1367\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1368\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1369\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1370\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1371\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m decoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;66;03m# T5v1.1 does not tie output word embeddings and thus does not require downscaling\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py:426\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    423\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    425\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[1;32m--> 426\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munpacked_inputs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:778\u001B[0m, in \u001B[0;36mTFT5MainLayer.call\u001B[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[0;32m    777\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_states,)\n\u001B[1;32m--> 778\u001B[0m layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_layer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_head_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mencoder_head_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[38;5;66;03m# layer_outputs is a tuple with:\u001B[39;00m\n\u001B[0;32m    794\u001B[0m \u001B[38;5;66;03m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001B[39;00m\n\u001B[0;32m    795\u001B[0m hidden_states, present_key_value_state \u001B[38;5;241m=\u001B[39m layer_outputs[:\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:567\u001B[0m, in \u001B[0;36mTFT5Block.call\u001B[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    565\u001B[0m     self_attn_past_key_value, cross_attn_past_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 567\u001B[0m self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    572\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    574\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    577\u001B[0m hidden_states, present_key_value_state \u001B[38;5;241m=\u001B[39m self_attention_outputs[:\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    578\u001B[0m attention_outputs \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m2\u001B[39m:]  \u001B[38;5;66;03m# Keep self-attention outputs and relative position weights\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:457\u001B[0m, in \u001B[0;36mTFT5LayerSelfAttention.call\u001B[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    447\u001B[0m     hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    454\u001B[0m     training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    455\u001B[0m ):\n\u001B[0;32m    456\u001B[0m     normed_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(hidden_states)\n\u001B[1;32m--> 457\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSelfAttention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnormed_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    467\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attention_output[\u001B[38;5;241m0\u001B[39m], training\u001B[38;5;241m=\u001B[39mtraining)\n\u001B[0;32m    468\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (hidden_states,) \u001B[38;5;241m+\u001B[39m attention_output[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:396\u001B[0m, in \u001B[0;36mTFT5Attention.call\u001B[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, training, output_attentions)\u001B[0m\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    393\u001B[0m         \u001B[38;5;66;03m# we might have a padded past structure, in which case we want to fetch the position bias slice\u001B[39;00m\n\u001B[0;32m    394\u001B[0m         \u001B[38;5;66;03m# right after the most recently filled past index\u001B[39;00m\n\u001B[0;32m    395\u001B[0m         most_recently_filled_past_index \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_max(tf\u001B[38;5;241m.\u001B[39mwhere(past_key_value[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, :, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0.0\u001B[39m))\n\u001B[1;32m--> 396\u001B[0m         position_bias \u001B[38;5;241m=\u001B[39m \u001B[43mdynamic_slice\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[43m            \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmost_recently_filled_past_index\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[43m            \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_seq_length\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    403\u001B[0m     position_bias \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(position_bias, dtype\u001B[38;5;241m=\u001B[39mmask\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\ops\\gen_xla_ops.py:1197\u001B[0m, in \u001B[0;36mxla_dynamic_slice\u001B[1;34m(input, start_indices, size_indices, name)\u001B[0m\n\u001B[0;32m   1195\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m _result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[0;32m   1196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m-> 1197\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m xla_dynamic_slice_eager_fallback(\n\u001B[0;32m   1198\u001B[0m       \u001B[38;5;28minput\u001B[39m, start_indices, size_indices, name\u001B[38;5;241m=\u001B[39mname, ctx\u001B[38;5;241m=\u001B[39m_ctx)\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_SymbolicException:\n\u001B[0;32m   1200\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\ops\\gen_xla_ops.py:1249\u001B[0m, in \u001B[0;36mxla_dynamic_slice_eager_fallback\u001B[1;34m(input, start_indices, size_indices, name, ctx)\u001B[0m\n\u001B[0;32m   1247\u001B[0m _inputs_flat \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28minput\u001B[39m, start_indices, size_indices]\n\u001B[0;32m   1248\u001B[0m _attrs \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m, _attr_T, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTindices\u001B[39m\u001B[38;5;124m\"\u001B[39m, _attr_Tindices)\n\u001B[1;32m-> 1249\u001B[0m _result \u001B[38;5;241m=\u001B[39m \u001B[43m_execute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mXlaDynamicSlice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_inputs_flat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1250\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_attrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _execute\u001B[38;5;241m.\u001B[39mmust_record_gradient():\n\u001B[0;32m   1252\u001B[0m   _execute\u001B[38;5;241m.\u001B[39mrecord_gradient(\n\u001B[0;32m   1253\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mXlaDynamicSlice\u001B[39m\u001B[38;5;124m\"\u001B[39m, _inputs_flat, _attrs, _result)\n",
      "\u001B[1;31mUnimplementedError\u001B[0m: Exception encountered when calling layer \"SelfAttention\" \"                 f\"(type TFT5Attention).\n\n{{function_node __wrapped__XlaDynamicSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Could not find compiler for platform Host: NOT_FOUND: could not find registered compiler for platform Host -- was support for that platform linked in? [Op:XlaDynamicSlice]\n\nCall arguments received by layer \"SelfAttention\" \"                 f\"(type TFT5Attention):\n  • hidden_states=tf.Tensor(shape=(1, 1, 512), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 1, 1, 2), dtype=float32)\n  • key_value_states=None\n  • position_bias=None\n  • past_key_value=('tf.Tensor(shape=(1, 8, 1, 64), dtype=float32)', 'tf.Tensor(shape=(1, 8, 1, 64), dtype=float32)')\n  • layer_head_mask=None\n  • query_length=None\n  • use_cache=True\n  • training=False\n  • output_attentions=False"
     ]
    }
   ],
   "source": [
    "output_y = []\n",
    "for sentence in test_x:\n",
    "    pred = decode_with_transformer(sentence, tokenizer, model)\n",
    "    output_y.append(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "input_x = test_x\n",
    "output_y_gt = test_y\n",
    "output_y_pred = output_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "for in_en, gt_es, pred_es in zip(input_x, output_y_gt, output_y_pred):\n",
    "        print(f'Input sentence: {in_en}')\n",
    "        print(f'GT translation: {gt_es}')\n",
    "        print(f'Pred translation: {pred_es}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f10bb063903c4f86a6c372430da44dff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84bd159cf9764f2c93d75555cd2988c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "845fb72cf8b541ad9d889b8d8d256ed2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[85], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m metric \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_y_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreferences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_y_gt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m score \u001B[38;5;241m=\u001B[39m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\evaluate\\module.py:450\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    447\u001B[0m compute_kwargs \u001B[38;5;241m=\u001B[39m {k: kwargs[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m--> 450\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_batch(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finalize()\n\u001B[0;32m    453\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\evaluate\\module.py:509\u001B[0m, in \u001B[0;36mEvaluationModule.add_batch\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    507\u001B[0m batch \u001B[38;5;241m=\u001B[39m {input_name: batch[input_name] \u001B[38;5;28;01mfor\u001B[39;00m input_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 509\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselected_feature_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_infer_feature_from_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_writer()\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\evaluate\\module.py:590\u001B[0m, in \u001B[0;36mEvaluationModule._infer_feature_from_batch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    588\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 590\u001B[0m     example \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m([(k, v[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()])\n\u001B[0;32m    591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_infer_feature_from_example(example)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\evaluate\\module.py:590\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    588\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 590\u001B[0m     example \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m([(k, \u001B[43mv\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()])\n\u001B[0;32m    591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_infer_feature_from_example(example)\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "metric = load('bleu')\n",
    "results = metric.compute(predictions=output_y_pred, references=output_y_gt)\n",
    "score = results['bleu']\n",
    "print(f'BLEU score: {score}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "m_score = 0.0\n",
    "for hyp, ref in zip(output_y_gt[:20], output_y_pred):\n",
    "        m_score += round(meteor([word_tokenize(hyp)], word_tokenize(ref)), 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(m_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
