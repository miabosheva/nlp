{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Користејќи го моделот FLAN-T5 со техниката few-shot prompting за секој примерок \n",
    "од податочното множество за препознавање на навредлив текст одредете дали \n",
    "примерокот содржи навредлив текст или не. Испробајте со користење различен \n",
    "број на примероци (n = 1, 2, 3, 5, 10).\n",
    "Добиените предвидувања евалуирајте ги со метриките: точност\n",
    "(accuracy_score), прецизност (precision_score), одзив (recall_score) и F1-\n",
    "мерка (f1_score). Евалуацијата направете ја посебно за сите подмножества \n",
    "(подмножество за тренирање, валидација и тестирање)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/test_en.txt\"\n",
    "train_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/train_en.txt\"\n",
    "val_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/val_en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = pd.read_table(train_en_path).dropna()\n",
    "test_en = pd.read_table(test_en_path).dropna()\n",
    "val_en = pd.read_table(val_en_path).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_en, pd.concat([test_en, val_en])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_en['Sentence'].values.tolist()\n",
    "train_labels = train_en['Label'].values.tolist()\n",
    "test_samples = test_en['Sentence'].values.tolist()\n",
    "test_labels = test_en['Label'].values.tolist()\n",
    "val_samples = val_en['Sentence'].values.tolist()\n",
    "val_labels = val_en['Label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Method (Accuracy Score, Precision Score, Recall Score, F1 Score metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred, prompt_type, train_test_or_val):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(train_test_or_val + \": \" + prompt_type)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method, here we write the prompt message, specify the number of examples that go into the prompt message and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 1 else 'non-offensive'\n",
    "            result_example = f'Text: {example_text}\\nCategory: {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        prompt = f'{example}\\nBased on the above example, classify the text into offensive or non-offensive: {sample}'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_prediction(pred_label):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    pred_list = []\n",
    "\n",
    "    for pred in pred_label:\n",
    "        pred = re.sub(pattern, '', pred)\n",
    "        pred = pred.strip()\n",
    "        pred = pred.lower()\n",
    "        # print(pred)\n",
    "\n",
    "        if pred == \"non-offensive\":\n",
    "            pred = 0\n",
    "        elif pred == \"offensive\":\n",
    "            pred = 1\n",
    "\n",
    "        pred_list.append(pred)\n",
    "\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples(samples, labels, 1) \n",
    "    pred_labels_n_2 = prompt_with_number_of_examples(samples, labels, 2)\n",
    "    pred_labels_n_5 = prompt_with_number_of_examples(samples, labels, 5)\n",
    "    pred_labels_n_10 = prompt_with_number_of_examples(samples, labels, 10)\n",
    "\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: N 1\n",
      "Accuracy: 0.53\n",
      "Precision: 1.0\n",
      "Recall: 0.53\n",
      "F1 Score: 0.6928104575163399\n",
      "Train: N 2\n",
      "Accuracy: 0.51\n",
      "Precision: 1.0\n",
      "Recall: 0.51\n",
      "F1 Score: 0.6754966887417219\n",
      "Train: N 5\n",
      "Accuracy: 0.55\n",
      "Precision: 1.0\n",
      "Recall: 0.55\n",
      "F1 Score: 0.7096774193548387\n",
      "Train: N 10\n",
      "Accuracy: 0.45\n",
      "Precision: 1.0\n",
      "Recall: 0.45\n",
      "F1 Score: 0.6206896551724138\n"
     ]
    }
   ],
   "source": [
    "pred_labels_train = predict_with_few_shot_prompting(train_samples[:100], train_labels[:100], \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best performance: N 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mia\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: N 1\n",
      "Accuracy: 0.65\n",
      "Precision: 1.0\n",
      "Recall: 0.65\n",
      "F1 Score: 0.787878787878788\n",
      "Test: N 2\n",
      "Accuracy: 0.61\n",
      "Precision: 1.0\n",
      "Recall: 0.61\n",
      "F1 Score: 0.7577639751552795\n",
      "Test: N 5\n",
      "Accuracy: 0.41\n",
      "Precision: 1.0\n",
      "Recall: 0.41\n",
      "F1 Score: 0.5815602836879432\n",
      "Test: N 10\n",
      "Accuracy: 0.37\n",
      "Precision: 1.0\n",
      "Recall: 0.37\n",
      "F1 Score: 0.5401459854014599\n"
     ]
    }
   ],
   "source": [
    "pred_labels_test = predict_with_few_shot_prompting(test_samples[:100], test_labels[:100], \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mia\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: N 1\n",
      "Accuracy: 0.47\n",
      "Precision: 1.0\n",
      "Recall: 0.47\n",
      "F1 Score: 0.6394557823129251\n",
      "Val: N 2\n",
      "Accuracy: 0.44\n",
      "Precision: 1.0\n",
      "Recall: 0.44\n",
      "F1 Score: 0.6111111111111112\n",
      "Val: N 5\n",
      "Accuracy: 0.4\n",
      "Precision: 1.0\n",
      "Recall: 0.4\n",
      "F1 Score: 0.5714285714285715\n",
      "Val: N 10\n",
      "Accuracy: 0.33\n",
      "Precision: 1.0\n",
      "Recall: 0.33\n",
      "F1 Score: 0.49624060150375937\n"
     ]
    }
   ],
   "source": [
    "pred_labels_val = predict_with_few_shot_prompting(val_samples[:100], val_labels[:100], \"Val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испробајте ги следните prompts:\n",
    "1. „Here is a text: <text>, which is <label>. Classify the following text: <text> into\n",
    "<label1> or <label2>.“.\n",
    "Факултет за информатички науки и компјутерско инженерство\n",
    "Обработка на природни јазици 2\n",
    "2. „Here is a text: <text>, which is not <label>. Classify the following text: <text> into\n",
    "<label1> or <label2>.“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples_prompt_type_1(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 1 else 'non-offensive'\n",
    "            result_example = f'Here is a text: {example_text},\\nwhich is {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        offensive = 'offensive'\n",
    "        non_offensve = 'non-offensive'\n",
    "        prompt = f'{example}\\nClassify the following text: {sample} into {offensive} or {non_offensve}'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples_prompt_type_2(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 0 else 'non-offensive'\n",
    "            result_example = f'Here is a text: {example_text},\\nwhich is not {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        offensive = 'offensive'\n",
    "        non_offensve = 'non-offensive'\n",
    "        prompt = f'{example}\\nClassify the following text: {sample} into {offensive} or {non_offensve}'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting_prompt_type_1(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 1) \n",
    "    # pred_labels_n_2 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 2)\n",
    "    # pred_labels_n_5 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 5)\n",
    "    # pred_labels_n_10 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 10)\n",
    "    print(labels)\n",
    "    print(clean_prediction(pred_labels_n_1))\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting_propmpt_type_2(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 1) \n",
    "    # pred_labels_n_2 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 2)\n",
    "    # pred_labels_n_5 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 5)\n",
    "    # pred_labels_n_10 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 10)\n",
    "    \n",
    "    print(clean_prediction(pred_labels_n_1))\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    # evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "predict_with_few_shot_prompting_prompt_type_1(train_samples[:10], train_labels[:10], \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_with_few_shot_prompting_prompt_type_2(train_samples[:100], train_labels[:100], \"Train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
