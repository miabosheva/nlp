{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Користејќи го моделот FLAN-T5 со техниката few-shot prompting за секој примерок \n",
    "од податочното множество за препознавање на навредлив текст одредете дали \n",
    "примерокот содржи навредлив текст или не. Испробајте со користење различен \n",
    "број на примероци (n = 1, 2, 3, 5, 10).\n",
    "Добиените предвидувања евалуирајте ги со метриките: точност\n",
    "(accuracy_score), прецизност (precision_score), одзив (recall_score) и F1-\n",
    "мерка (f1_score). Евалуацијата направете ја посебно за сите подмножества \n",
    "(подмножество за тренирање, валидација и тестирање)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/test_en.txt\"\n",
    "train_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/train_en.txt\"\n",
    "val_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/val_en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = pd.read_table(train_en_path).dropna()\n",
    "test_en = pd.read_table(test_en_path).dropna()\n",
    "val_en = pd.read_table(val_en_path).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_en, pd.concat([test_en, val_en])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_en['Sentence'].values.tolist()\n",
    "train_labels = train_en['Label'].values.tolist()\n",
    "test_samples = test_en['Sentence'].values.tolist()\n",
    "test_labels = test_en['Label'].values.tolist()\n",
    "val_samples = val_en['Sentence'].values.tolist()\n",
    "val_labels = val_en['Label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Method (Accuracy Score, Precision Score, Recall Score, F1 Score metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred, prompt_type, train_test_or_val):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(train_test_or_val + \": \" + prompt_type)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method, here we write the prompt message, specify the number of examples that go into the prompt message and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 1 else 'non-offensive'\n",
    "            result_example = f'Text: {example_text}\\nCategory: {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        prompt = f'{example}\\nBased on the above example, classify the text into offensive or non-offensive: {sample}'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_prediction(pred_label):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    pred_list = []\n",
    "\n",
    "    for pred in pred_label:\n",
    "        pred = re.sub(pattern, '', pred)\n",
    "        pred = pred.strip()\n",
    "        pred = pred.lower()\n",
    "        # print(pred)\n",
    "\n",
    "        if pred == \"non-offensive\":\n",
    "            pred = 0\n",
    "        else:\n",
    "            pred = 1\n",
    "\n",
    "        pred_list.append(pred)\n",
    "\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples(samples, labels, 1) \n",
    "    pred_labels_n_2 = prompt_with_number_of_examples(samples, labels, 2)\n",
    "    pred_labels_n_5 = prompt_with_number_of_examples(samples, labels, 5)\n",
    "    pred_labels_n_10 = prompt_with_number_of_examples(samples, labels, 10)\n",
    "\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: N 1\n",
      "Accuracy: 0.53\n",
      "Precision: 1.0\n",
      "Recall: 0.53\n",
      "F1 Score: 0.6928104575163399\n",
      "Train: N 2\n",
      "Accuracy: 0.51\n",
      "Precision: 1.0\n",
      "Recall: 0.51\n",
      "F1 Score: 0.6754966887417219\n",
      "Train: N 5\n",
      "Accuracy: 0.55\n",
      "Precision: 1.0\n",
      "Recall: 0.55\n",
      "F1 Score: 0.7096774193548387\n",
      "Train: N 10\n",
      "Accuracy: 0.45\n",
      "Precision: 1.0\n",
      "Recall: 0.45\n",
      "F1 Score: 0.6206896551724138\n"
     ]
    }
   ],
   "source": [
    "pred_labels_train = predict_with_few_shot_prompting(train_samples[:100], train_labels[:100], \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best performance: N 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mia\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: N 1\n",
      "Accuracy: 0.65\n",
      "Precision: 1.0\n",
      "Recall: 0.65\n",
      "F1 Score: 0.787878787878788\n",
      "Test: N 2\n",
      "Accuracy: 0.61\n",
      "Precision: 1.0\n",
      "Recall: 0.61\n",
      "F1 Score: 0.7577639751552795\n",
      "Test: N 5\n",
      "Accuracy: 0.41\n",
      "Precision: 1.0\n",
      "Recall: 0.41\n",
      "F1 Score: 0.5815602836879432\n",
      "Test: N 10\n",
      "Accuracy: 0.37\n",
      "Precision: 1.0\n",
      "Recall: 0.37\n",
      "F1 Score: 0.5401459854014599\n"
     ]
    }
   ],
   "source": [
    "pred_labels_test = predict_with_few_shot_prompting(test_samples[:100], test_labels[:100], \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best performance: N 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mia\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: N 1\n",
      "Accuracy: 0.47\n",
      "Precision: 1.0\n",
      "Recall: 0.47\n",
      "F1 Score: 0.6394557823129251\n",
      "Val: N 2\n",
      "Accuracy: 0.44\n",
      "Precision: 1.0\n",
      "Recall: 0.44\n",
      "F1 Score: 0.6111111111111112\n",
      "Val: N 5\n",
      "Accuracy: 0.4\n",
      "Precision: 1.0\n",
      "Recall: 0.4\n",
      "F1 Score: 0.5714285714285715\n",
      "Val: N 10\n",
      "Accuracy: 0.33\n",
      "Precision: 1.0\n",
      "Recall: 0.33\n",
      "F1 Score: 0.49624060150375937\n"
     ]
    }
   ],
   "source": [
    "pred_labels_val = predict_with_few_shot_prompting(val_samples[:100], val_labels[:100], \"Val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best performance: N 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можеме да забележиме дека во овој вид на промпт: Text: text Category: category, имаме многу добри резултати но резултатите се влошуваат кога користиме повеќе примери. Со оглед на резултатите можеме да земеме некоја средина, и да искористиме 2 примери како најоптимална постапка.\n",
    "\n",
    "Во втората лабораториска задача ги добив следните резултати во евалуацијата:\n",
    "\n",
    "* Accuracy: 0.5138539042821159\n",
    "* Precision: 0.510556621880998\n",
    "* Recall: 0.6700251889168766\n",
    "* F1 Score: 0.579520697167756 \n",
    "\n",
    "Според ова можеме да заклучиме дека FLAN-T5 со few-shot prompting е далеку побрз и попрецизен од невронските секвенцијални мрежи со LSTM, Embedding и Dense слоеви."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испробајте ги следните prompts:\n",
    "1. „Here is a text: {text}, which is {label}. Classify the following text: {sample} into\n",
    "{label1} or {label2}.“\n",
    "2. „Here is a text: {text}, which is not {label}. Classify the following text: {sample} into\n",
    "{label1} or {label2}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples_prompt_type_1(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 1 else 'non-offensive'\n",
    "            result_example = f'Here is a text: {example_text}, which is {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        label1 = 'offensive'\n",
    "        label2 = 'non-offensive'\n",
    "        prompt = f'{example}\\nClassify the following text: {sample}, into {label1} or {label2}.'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "        # print(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples_prompt_type_2(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 0 else 'non-offensive'\n",
    "            result_example = f'Here is a text: {example_text},which is not {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        label1 = 'offensive'\n",
    "        label2 = 'non-offensive'\n",
    "        prompt = f'{example}\\nClassify the following text: {sample} into {label1} or {label2}'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting_prompt_type_1(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 1) \n",
    "    pred_labels_n_2 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 2)\n",
    "    pred_labels_n_5 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 5)\n",
    "    pred_labels_n_10 = prompt_with_number_of_examples_prompt_type_1(samples, labels, 10)\n",
    "\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting_prompt_type_2(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 1) \n",
    "    pred_labels_n_2 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 2)\n",
    "    pred_labels_n_5 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 5)\n",
    "    pred_labels_n_10 = prompt_with_number_of_examples_prompt_type_2(samples, labels, 10)\n",
    "    \n",
    "    # print(clean_prediction(pred_labels_n_1))\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here is a text: Pussy nobody asked for your input., which is offensive']\n",
      "Classify the following text: Pussy nobody asked for your input. into offensive or non-offensive\n",
      "<pad> Non-offensive</s>\n",
      "['Here is a text: Pussy nobody asked for your input., which is offensive']\n",
      "Classify the following text: Ok, this makes no sense. This will create vigilantes. And I hope they put this savage animal down before he hurts anyone else. into offensive or non-offensive\n",
      "<pad> Non-offensive</s>\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "# predict_with_few_shot_prompting_prompt_type_1(train_samples[:2], train_labels[:2], \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воочувам дека моделот тешко предвидува со еден пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mia\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: N 1\n",
      "Accuracy: 0.27\n",
      "Precision: 1.0\n",
      "Recall: 0.27\n",
      "F1 Score: 0.4251968503937008\n",
      "Train: N 2\n",
      "Accuracy: 0.22\n",
      "Precision: 1.0\n",
      "Recall: 0.22\n",
      "F1 Score: 0.36065573770491804\n",
      "Train: N 5\n",
      "Accuracy: 0.39\n",
      "Precision: 1.0\n",
      "Recall: 0.39\n",
      "F1 Score: 0.5611510791366906\n",
      "Train: N 10\n",
      "Accuracy: 0.41\n",
      "Precision: 1.0\n",
      "Recall: 0.41\n",
      "F1 Score: 0.5815602836879432\n"
     ]
    }
   ],
   "source": [
    "predict_with_few_shot_prompting_prompt_type_1(train_samples[:100], train_labels[:100], \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best performance: N 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mia\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: N 1\n",
      "Accuracy: 0.1\n",
      "Precision: 1.0\n",
      "Recall: 0.1\n",
      "F1 Score: 0.18181818181818182\n",
      "Train: N 2\n",
      "Accuracy: 0.05\n",
      "Precision: 1.0\n",
      "Recall: 0.05\n",
      "F1 Score: 0.09523809523809523\n",
      "Train: N 5\n",
      "Accuracy: 0.06\n",
      "Precision: 1.0\n",
      "Recall: 0.06\n",
      "F1 Score: 0.11320754716981131\n",
      "Train: N 10\n",
      "Accuracy: 0.05\n",
      "Precision: 1.0\n",
      "Recall: 0.05\n",
      "F1 Score: 0.09523809523809523\n"
     ]
    }
   ],
   "source": [
    "predict_with_few_shot_prompting_prompt_type_2(train_samples[:100], train_labels[:100], \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best performance: N 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Првиот вид на промпт: „Here is a text: {text}, which is {label}. Classify the following text: {sample} into\n",
    "{label1} or {label2}.“ има подобри резултати од вториот.\n",
    "- Со најдобар перформанс со f-1 резултат од 0.58, можам да кажам дека овој промпт е значително послаб од оној што го испробавме прв (тој имаше f-1 резултат од 0.78).\n",
    "- За разлика од првиот промпт, овој станува подобар со повеќе примери.\n",
    "\n",
    "2. „Here is a text: {text}, which is not {label}. Classify the following text: {sample} into\n",
    "{label1} or {label2}.\"\n",
    "- Овој тип на промпт го има најлошиот резултат во целото истражување. Иако се подобрува со зголемување на бројот на примерите, сепак не е доволно добар за да се спореди со претходните два. \n",
    "\n",
    "Заклучоци од овој обид:\n",
    "- Најдобри резултати добиваат промптови кои што се прецизни и недвомислени. \n",
    "\n",
    "Пример:\n",
    "Промптот од тип 1 имаше подобар резултат од тип 2, бидејќи тип 2 користеше негирање \"which is not {label}\", место \"which is {label}\".\n",
    "- Промптови каде што поедноставно се специфицира, класифицира и се даваат појасни инструкции се поуспешни.\n",
    "\n",
    "Пример: Here is a text: {text}, which is {label} може да се поедностави во видот: Text: {Text} Category: {Category}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
