{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Користејќи го моделот FLAN-T5 со техниката few-shot prompting за секој примерок \n",
    "од податочното множество за препознавање на навредлив текст одредете дали \n",
    "примерокот содржи навредлив текст или не. Испробајте со користење различен \n",
    "број на примероци (n = 1, 2, 3, 5, 10).\n",
    "Добиените предвидувања евалуирајте ги со метриките: точност\n",
    "(accuracy_score), прецизност (precision_score), одзив (recall_score) и F1-\n",
    "мерка (f1_score). Евалуацијата направете ја посебно за сите подмножества \n",
    "(подмножество за тренирање, валидација и тестирање).\n",
    "Испробајте ги следните prompts:\n",
    "1. „Here is a text: <text>, which is <label>. Classify the following text: <text> into\n",
    "<label1> or <label2>.“.\n",
    "2. „Here is a text: <text>, which is not <label>. Classify the following text: <text> into\n",
    "<label1> or <label2>.“.\n",
    "Дали овој модел е подобар од моделите во втората лабораториска вежба?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/test_en.txt\"\n",
    "train_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/train_en.txt\"\n",
    "val_en_path = \"C:/Users/Mia/Desktop/FINKI/NLP/nlp/data/offensive text detection/val_en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = pd.read_table(train_en_path).dropna()\n",
    "test_en = pd.read_table(test_en_path).dropna()\n",
    "val_en = pd.read_table(val_en_path).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_en, pd.concat([test_en, val_en])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_en['Sentence'].values.tolist()\n",
    "train_labels = train_en['Label'].values.tolist()\n",
    "test_samples = test_en['Sentence'].values.tolist()\n",
    "test_labels = test_en['Label'].values.tolist()\n",
    "val_samples = val_en['Sentence'].values.tolist()\n",
    "val_labels = val_en['Label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred, prompt_type, train_test_or_val):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(train_test_or_val + \": \" + prompt_type)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_number_of_examples(samples, labels, no_of_examples):\n",
    "    pred_labels = []\n",
    "\n",
    "    for sample, label in zip(samples, labels):\n",
    "        example = []\n",
    "       \n",
    "        for i in range(no_of_examples):\n",
    "            example_text = samples[i]\n",
    "            example_label = 'offensive' if labels[i] == 1 else 'non-offensive'\n",
    "            result_example = f'Text: {example_text}\\nCategory: {example_label}'\n",
    "            example.append(result_example)\n",
    "\n",
    "        prompt = f'{example}\\nBased on the above example, classify the text into offensive or non-offensive: {sample}'\n",
    "        \n",
    "        # print(prompt)\n",
    "\n",
    "        input_data = tokenizer(prompt, return_tensors='pt')\n",
    "        input_ids = input_data.input_ids\n",
    "        \n",
    "        output = model.generate(input_ids)\n",
    "        pred_label = tokenizer.decode(output[0])\n",
    "\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_prediction(pred_label):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    pred_list = []\n",
    "\n",
    "    for pred in pred_label:\n",
    "        pred = re.sub(pattern, '', pred)\n",
    "        pred = pred.strip()\n",
    "        # print(pred)\n",
    "\n",
    "        if pred == \"non-offensive\":\n",
    "            pred = 0\n",
    "        elif pred == \"offensive\":\n",
    "            pred = 1\n",
    "\n",
    "        pred_list.append(pred)\n",
    "\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_few_shot_prompting(samples, labels, train_test_or_val):\n",
    "\n",
    "    pred_labels_n_1 = prompt_with_number_of_examples(samples, labels, 1) \n",
    "    pred_labels_n_2 = prompt_with_number_of_examples(samples, labels, 2)\n",
    "    pred_labels_n_5 = prompt_with_number_of_examples(samples, labels, 5)\n",
    "    pred_labels_n_10 = prompt_with_number_of_examples(samples, labels, 10)\n",
    "\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_1), \"N 1\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_2), \"N 2\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_5), \"N 5\", train_test_or_val)\n",
    "    evaluate(labels, clean_prediction(pred_labels_n_10), \"N 10\", train_test_or_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_train = predict_with_few_shot_prompting(train_samples[:100], train_labels[:100], \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text: So maybe you should be more retarded.\\nCategory: offensive']\n",
      "Based on the above example, classify the text into offensive or non-offensive: So maybe you should be more retarded.\n",
      "['Text: So maybe you should be more retarded.\\nCategory: offensive']\n",
      "Based on the above example, classify the text into offensive or non-offensive: THERES A MEGATHREAD FOR VACCINE OR COVID RELATED TOPICS. DON'T TALK ABOUT THAT SHIT HERE IDIOT!\n",
      "['Text: So maybe you should be more retarded.\\nCategory: offensive', \"Text: THERES A MEGATHREAD FOR VACCINE OR COVID RELATED TOPICS. DON'T TALK ABOUT THAT SHIT HERE IDIOT!\\nCategory: offensive\"]\n",
      "Based on the above example, classify the text into offensive or non-offensive: So maybe you should be more retarded.\n",
      "['Text: So maybe you should be more retarded.\\nCategory: offensive', \"Text: THERES A MEGATHREAD FOR VACCINE OR COVID RELATED TOPICS. DON'T TALK ABOUT THAT SHIT HERE IDIOT!\\nCategory: offensive\"]\n",
      "Based on the above example, classify the text into offensive or non-offensive: THERES A MEGATHREAD FOR VACCINE OR COVID RELATED TOPICS. DON'T TALK ABOUT THAT SHIT HERE IDIOT!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_labels_test \u001b[38;5;241m=\u001b[39m predict_with_few_shot_prompting(test_samples[:\u001b[38;5;241m2\u001b[39m], test_labels[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mpredict_with_few_shot_prompting\u001b[1;34m(samples, labels, train_test_or_val)\u001b[0m\n\u001b[0;32m      3\u001b[0m pred_labels_n_1 \u001b[38;5;241m=\u001b[39m prompt_with_number_of_examples(samples, labels, \u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m      4\u001b[0m pred_labels_n_2 \u001b[38;5;241m=\u001b[39m prompt_with_number_of_examples(samples, labels, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m pred_labels_n_5 \u001b[38;5;241m=\u001b[39m prompt_with_number_of_examples(samples, labels, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      6\u001b[0m pred_labels_n_10 \u001b[38;5;241m=\u001b[39m prompt_with_number_of_examples(samples, labels, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      8\u001b[0m evaluate(labels, clean_prediction(pred_labels_n_1), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_test_or_val)\n",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m, in \u001b[0;36mprompt_with_number_of_examples\u001b[1;34m(samples, labels, no_of_examples)\u001b[0m\n\u001b[0;32m      5\u001b[0m example \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_of_examples):\n\u001b[1;32m----> 8\u001b[0m     example_text \u001b[38;5;241m=\u001b[39m samples[i]\n\u001b[0;32m      9\u001b[0m     example_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffensive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-offensive\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m     result_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCategory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred_labels_test = predict_with_few_shot_prompting(test_samples[:100], test_labels[:100], \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_with_few_shot_prompting' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_labels_val \u001b[38;5;241m=\u001b[39m predict_with_few_shot_prompting(val_samples[:\u001b[38;5;241m100\u001b[39m], val_labels[:\u001b[38;5;241m100\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_with_few_shot_prompting' is not defined"
     ]
    }
   ],
   "source": [
    "pred_labels_val = predict_with_few_shot_prompting(val_samples[:100], val_labels[:100], \"Val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 1.0\n",
      "Recall: 0.53\n",
      "F1 Score: 0.6928104575163399\n"
     ]
    }
   ],
   "source": [
    "evaluate(train_labels[:100], clean_prediction(pred_labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "Precision: 1.0\n",
      "Recall: 0.59\n",
      "F1 Score: 0.7421383647798743\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_labels[:100], clean_prediction(pred_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 1.0\n",
      "Recall: 0.53\n",
      "F1 Score: 0.6928104575163399\n"
     ]
    }
   ],
   "source": [
    "evaluate(val_labels[:100], clean_prediction(pred_labels_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
